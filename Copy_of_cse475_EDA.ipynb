{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irfan-Riyad/-Customer-Segmentation-Using-RFM-Analysis/blob/main/Copy_of_cse475_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGSRqt_DGqOL",
        "outputId": "859de7fd-6722-4602-b192-bcc9bc265aac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagehash) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from imagehash) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imagehash) (1.16.2)\n",
            "Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m276.5/296.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imagehash\n",
            "Successfully installed imagehash-4.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip -q install imagehash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6tZRdzqVGpKz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from skimage import img_as_float\n",
        "from scipy.stats import skew, kurtosis\n",
        "from imagehash import phash\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_3EHGBCGxcJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf2Taq4CGzp4"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content/drive/MyDrive/Original_Data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ_5H7jAG-o0"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset path:\", data_dir)\n",
        "print(\"Exists?\", os.path.exists(data_dir))\n",
        "print(\"Subfolders:\", os.listdir(data_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Fj8Zvc9DxRY"
      },
      "outputs": [],
      "source": [
        "# ==========================\n",
        "# 0️⃣ Copy dataset locally (optional but faster)\n",
        "# ==========================\n",
        "LOCAL_PATH = \"/content/Original_Data\"\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Original_Data\"\n",
        "\n",
        "if not os.path.exists(LOCAL_PATH):\n",
        "    shutil.copytree(DRIVE_PATH, LOCAL_PATH)\n",
        "    print(f\"✅ Dataset copied locally to {LOCAL_PATH}\")\n",
        "else:\n",
        "    print(f\"✅ Local dataset already exists at {LOCAL_PATH}\")\n",
        "\n",
        "# ==========================\n",
        "# 1️⃣ Detect classes from folder names\n",
        "# ==========================\n",
        "BASE_PATH = LOCAL_PATH\n",
        "available_classes = [f for f in os.listdir(BASE_PATH) if os.path.isdir(os.path.join(BASE_PATH, f))]\n",
        "print(f\"Available class folders: {available_classes}\")\n",
        "\n",
        "if not available_classes:\n",
        "    raise ValueError(\"❌ No class folders found! Check dataset path.\")\n",
        "\n",
        "CLASSES = available_classes\n",
        "print(f\"Using class folders: {CLASSES}\")\n",
        "\n",
        "# ==========================\n",
        "# 2️⃣ Helper function to compute stats\n",
        "# ==========================\n",
        "def compute_image_stats(image):\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "\n",
        "    arr = np.asarray(image, dtype=np.float32) / 255.0\n",
        "    hsv = np.asarray(image.convert(\"HSV\"), dtype=np.float32) / 255.0\n",
        "\n",
        "    stats = {}\n",
        "    for i, c in enumerate(['R', 'G', 'B']):\n",
        "        stats[f'{c}_mean'] = arr[:, :, i].mean()\n",
        "        stats[f'{c}_std'] = arr[:, :, i].std()\n",
        "    for i, c in enumerate(['H', 'S', 'V']):\n",
        "        stats[f'{c}_mean'] = hsv[:, :, i].mean()\n",
        "        stats[f'{c}_std'] = hsv[:, :, i].std()\n",
        "\n",
        "    stats['brightness'] = hsv[:, :, 2].mean()\n",
        "    stats['contrast'] = arr.std(axis=(0,1)).mean()\n",
        "\n",
        "    s = hsv[:, :, 1]\n",
        "    stats['sat_clip_low'] = np.mean(s < 0.05)\n",
        "    stats['sat_clip_high'] = np.mean(s > 0.95)\n",
        "\n",
        "    stats['width'], stats['height'] = image.size\n",
        "    stats['aspect_ratio'] = image.width / image.height\n",
        "\n",
        "    return stats\n",
        "\n",
        "# ==========================\n",
        "# 3️⃣ Process all images (optimized)\n",
        "# ==========================\n",
        "rows = []\n",
        "MAX_IMAGES = None  # Optional: test run (e.g., 200)\n",
        "\n",
        "for cls in CLASSES:\n",
        "    folder = os.path.join(BASE_PATH, cls)\n",
        "    img_paths = glob.glob(os.path.join(folder, \"*.*\"))\n",
        "    if MAX_IMAGES:\n",
        "        img_paths = img_paths[:MAX_IMAGES]\n",
        "\n",
        "    if not img_paths:\n",
        "        print(f\"⚠️ No images found in folder: {folder}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing {len(img_paths)} images in class '{cls}'...\")\n",
        "    for path in tqdm(img_paths, desc=f\"{cls} images\"):\n",
        "        if not path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
        "            continue\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            stats = compute_image_stats(img)\n",
        "            stats[\"class\"] = cls\n",
        "            stats[\"filename\"] = os.path.basename(path)\n",
        "            rows.append(stats)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "if not rows:\n",
        "    raise ValueError(\"❌ No valid images found! Check dataset path or folder structure.\")\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(f\"✅ Total images processed: {len(df)}\")\n",
        "display(df.head())\n",
        "\n",
        "# ==========================\n",
        "# 4️⃣ Sample histogram visualization\n",
        "# ==========================\n",
        "sample_row = df.iloc[0]\n",
        "sample_img_path = os.path.join(BASE_PATH, sample_row[\"class\"], sample_row[\"filename\"])\n",
        "sample_img = Image.open(sample_img_path)\n",
        "arr = np.asarray(sample_img)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "for i,color in enumerate(['r','g','b']):\n",
        "    plt.hist(arr[:,:,i].ravel(), bins=256, color=color, alpha=0.5, label=color.upper())\n",
        "plt.title(\"RGB Histogram (Sample)\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "hsv = np.asarray(sample_img.convert(\"HSV\"))\n",
        "plt.figure(figsize=(12,4))\n",
        "for i,color in enumerate(['h','s','v']):\n",
        "    plt.hist(hsv[:,:,i].ravel(), bins=256, alpha=0.5, label=color.upper())\n",
        "plt.title(\"HSV Histogram (Sample)\")\n",
        "plt.legend(); plt.show()\n",
        "\n",
        "# ==========================\n",
        "# 5️⃣ Per-class summary\n",
        "# ==========================\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "summary = df.groupby(\"class\")[numeric_cols].agg([\"mean\",\"std\"])\n",
        "print(\"=== Per-Class Mean & STD ===\")\n",
        "display(summary)\n",
        "\n",
        "# ==========================\n",
        "# 6️⃣ Brightness vs Contrast\n",
        "# ==========================\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.scatterplot(data=df, x=\"brightness\", y=\"contrast\", hue=\"class\", alpha=0.7)\n",
        "plt.title(\"Brightness vs Contrast Spread\"); plt.show()\n",
        "\n",
        "# ==========================\n",
        "# 7️⃣ Saturation clipping\n",
        "# ==========================\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.boxplot(data=df, x=\"class\", y=\"sat_clip_high\")\n",
        "plt.title(\"Saturation Clipping (High values)\"); plt.show()\n",
        "\n",
        "# ==========================\n",
        "# 8️⃣ Resolution & Aspect Ratio\n",
        "# ==========================\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.histplot(data=df, x=\"width\", bins=30, hue=\"class\", element=\"step\")\n",
        "plt.title(\"Width Distribution per Class\"); plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.histplot(data=df, x=\"height\", bins=30, hue=\"class\", element=\"step\")\n",
        "plt.title(\"Height Distribution per Class\"); plt.show()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.histplot(data=df, x=\"aspect_ratio\", bins=30, hue=\"class\", element=\"step\")\n",
        "plt.title(\"Aspect Ratio Distribution per Class\"); plt.show()\n",
        "\n",
        "# ==========================\n",
        "# 9️⃣ Suggested resize/padding\n",
        "# ==========================\n",
        "mean_w, mean_h = df['width'].mean(), df['height'].mean()\n",
        "print(f\"📏 Suggested resize target: ({int(mean_w)}x{int(mean_h)})\")\n",
        "print(\"\"\"\n",
        "💡 Suggested Strategy:\n",
        "- Resize all images to fixed size (e.g., 256×256)\n",
        "- Maintain aspect ratio by padding (black or mean color)\n",
        "- For CNNs, random crop + resize augmentation recommended\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFkZjxSgHQmN"
      },
      "outputs": [],
      "source": [
        "# Functions\n",
        "def laplacian_variance(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return None\n",
        "    lap = cv2.Laplacian(img, cv2.CV_64F)\n",
        "    return lap.var()\n",
        "\n",
        "def noise_proxy(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return None\n",
        "    blurred = cv2.GaussianBlur(img, (3, 3), 0)\n",
        "    noise = img.astype(\"float\") - blurred.astype(\"float\")\n",
        "    return noise.var()\n",
        "\n",
        "# Base path to your dataset in Google Drive\n",
        "# Example: '/content/drive/MyDrive/ArsenicSkinImageBD/Original'\n",
        "base_path = '/content/drive/MyDrive/Original_Data'\n",
        "categories = ['infacted', 'not_infacted']\n",
        "\n",
        "data = []\n",
        "\n",
        "# Loop through each folder and image\n",
        "for category in categories:\n",
        "    folder_path = os.path.join(base_path, category)\n",
        "    for filename in tqdm(os.listdir(folder_path)):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        lap_var = laplacian_variance(img_path)\n",
        "        noise_val = noise_proxy(img_path)\n",
        "\n",
        "        data.append({\n",
        "            'filename': filename,\n",
        "            'category': category,\n",
        "            'laplacian_variance': lap_var,\n",
        "            'noise_proxy': noise_val\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV in Colab\n",
        "output_csv = '/content/laplacian_noise_results.csv'\n",
        "df.to_csv(output_csv, index=False)\n",
        "print(f\"CSV saved to {output_csv}\")\n",
        "\n",
        "# Show mean stats by category\n",
        "print(df.groupby('category')[['laplacian_variance', 'noise_proxy']].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pN5apU0AHdfy"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# 📘 Image Analysis & Duplicate Detection (GitHub-Safe Version)\n",
        "# ==============================\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os, glob, itertools, shutil\n",
        "import pandas as pd\n",
        "from tqdm import tqdm  # ✅ safer than tqdm.notebook for GitHub/nbconvert\n",
        "\n",
        "# -------------------------------\n",
        "# 0️⃣ Image Folder Path\n",
        "# -------------------------------\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/Original_Data\"\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Gray-World White Balance Function\n",
        "# -------------------------------\n",
        "def gray_world_correction(pil_image):\n",
        "    \"\"\"Apply Gray-World white balance correction to an image.\"\"\"\n",
        "    if pil_image.mode != 'RGB':\n",
        "        pil_image = pil_image.convert('RGB')\n",
        "    arr = np.asarray(pil_image).astype(np.float32)\n",
        "\n",
        "    mean_r = arr[:, :, 0].mean()\n",
        "    mean_g = arr[:, :, 1].mean()\n",
        "    mean_b = arr[:, :, 2].mean()\n",
        "    mean_gray = (mean_r + mean_g + mean_b) / 3.0\n",
        "\n",
        "    # Scale each channel\n",
        "    scale_r = mean_gray / mean_r\n",
        "    scale_g = mean_gray / mean_g\n",
        "    scale_b = mean_gray / mean_b\n",
        "\n",
        "    arr[:, :, 0] = np.clip(arr[:, :, 0] * scale_r, 0, 255)\n",
        "    arr[:, :, 1] = np.clip(arr[:, :, 1] * scale_g, 0, 255)\n",
        "    arr[:, :, 2] = np.clip(arr[:, :, 2] * scale_b, 0, 255)\n",
        "\n",
        "    return Image.fromarray(arr.astype(np.uint8))\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Perceptual Hashing (aHash + dHash)\n",
        "# -------------------------------\n",
        "def ahash(image, hash_size=8):\n",
        "    \"\"\"Average Hash (aHash) - converts image to binary hash string.\"\"\"\n",
        "    image = image.convert('L').resize((hash_size, hash_size), Image.Resampling.LANCZOS)\n",
        "    pixels = np.array(image)\n",
        "    avg = pixels.mean()\n",
        "    return ''.join(['1' if p > avg else '0' for p in pixels.flatten()])\n",
        "\n",
        "def dhash(image, hash_size=8):\n",
        "    \"\"\"Difference Hash (dHash) - compares pixel gradients.\"\"\"\n",
        "    image = image.convert('L').resize((hash_size + 1, hash_size), Image.Resampling.LANCZOS)\n",
        "    pixels = np.array(image)\n",
        "    diff = pixels[:, 1:] > pixels[:, :-1]\n",
        "    return ''.join(['1' if v else '0' for v in diff.flatten()])\n",
        "\n",
        "def hamming_distance(hash1, hash2):\n",
        "    \"\"\"Calculate Hamming distance between two binary hash strings.\"\"\"\n",
        "    return sum(c1 != c2 for c1, c2 in zip(hash1, hash2))\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Collect All Image Paths\n",
        "# -------------------------------\n",
        "image_paths = glob.glob(os.path.join(IMAGE_FOLDER, '**', '*.*'), recursive=True)\n",
        "image_paths = [p for p in image_paths if p.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
        "\n",
        "print(f\"🔍 Total image paths found: {len(image_paths)}\")\n",
        "if len(image_paths) > 0:\n",
        "    print(\"First few image paths:\", image_paths[:3])\n",
        "\n",
        "# -------------------------------\n",
        "# 4️⃣ Process Each Image\n",
        "# -------------------------------\n",
        "rows = []\n",
        "for img_path in tqdm(image_paths, desc=\"Processing images\"):\n",
        "    try:\n",
        "        img = Image.open(img_path)\n",
        "        wb_img = gray_world_correction(img)\n",
        "        row = {\n",
        "            \"filename\": os.path.basename(img_path),\n",
        "            \"path\": img_path,\n",
        "            \"width\": img.width,\n",
        "            \"height\": img.height,\n",
        "            \"ahash\": ahash(img),\n",
        "            \"dhash\": dhash(img)\n",
        "        }\n",
        "        rows.append(row)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error processing {img_path}: {e}\")\n",
        "\n",
        "print(\"✅ Number of rows collected:\", len(rows))\n",
        "if len(rows) > 0:\n",
        "    print(\"Sample keys:\", list(rows[0].keys()))\n",
        "\n",
        "# -------------------------------\n",
        "# 5️⃣ Create DataFrame (Safe)\n",
        "# -------------------------------\n",
        "if not rows:\n",
        "    print(\"❌ No valid image data found — check your IMAGE_FOLDER path or image formats.\")\n",
        "else:\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    if \"filename\" in df.columns:\n",
        "        df = df.sort_values(\"filename\").reset_index(drop=True)\n",
        "        print(\"\\n✅ === Image Analysis Results ===\")\n",
        "        display(df.head())\n",
        "    else:\n",
        "        print(\"\\n⚠️ 'filename' column missing — showing raw DataFrame instead.\")\n",
        "        display(df)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 6️⃣ Duplicate Detection\n",
        "    # -------------------------------\n",
        "    print(\"\\n🔁 Detecting duplicates (this may take some time)...\")\n",
        "\n",
        "    duplicates = []\n",
        "    threshold = 5  # max hamming distance to consider duplicate\n",
        "\n",
        "    for (i1, row1), (i2, row2) in itertools.combinations(df.iterrows(), 2):\n",
        "        hd_ahash = hamming_distance(row1['ahash'], row2['ahash'])\n",
        "        hd_dhash = hamming_distance(row1['dhash'], row2['dhash'])\n",
        "        if hd_ahash <= threshold and hd_dhash <= threshold:\n",
        "            duplicates.append({\n",
        "                \"file1\": row1['filename'],\n",
        "                \"file2\": row2['filename'],\n",
        "                \"ahash_dist\": hd_ahash,\n",
        "                \"dhash_dist\": hd_dhash\n",
        "            })\n",
        "\n",
        "    dup_df = pd.DataFrame(duplicates)\n",
        "    print(\"\\n✅ === Duplicate Image Report ===\")\n",
        "    if not dup_df.empty:\n",
        "        display(dup_df)\n",
        "    else:\n",
        "        print(\"No duplicates detected ✅\")\n",
        "\n",
        "print(\"\\n🎯 Analysis completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JBc-uDqXVkr"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# STEP 1: Set dataset path & verify\n",
        "# ===============================\n",
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/Original_Data'  #  Adjust if needed\n",
        "\n",
        "print(\"Dataset path:\", data_dir)\n",
        "print(\"Exists?\", os.path.exists(data_dir))\n",
        "print(\"Subfolders:\", os.listdir(data_dir))\n",
        "\n",
        "# ===============================\n",
        "# STEP 2: Recursively scan for JPG files & build DataFrame\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Accept both lower & upper case extensions\n",
        "valid_ext = ('.jpg', '.jpeg', '.JPG', '.JPEG', '.png', '.PNG')\n",
        "\n",
        "# Recursively collect file paths\n",
        "file_paths = glob.glob(os.path.join(data_dir, '**', '*.*'), recursive=True)\n",
        "print(\"Total files found recursively:\", len(file_paths))\n",
        "\n",
        "rows = []\n",
        "for fpath in file_paths:\n",
        "    if fpath.endswith(valid_ext):\n",
        "        # Label = parent folder name (e.g., 'infacted', 'not_infacted')\n",
        "        label = os.path.basename(os.path.dirname(fpath))\n",
        "        # Group ID = filename stem or prefix before underscore\n",
        "        fname = os.path.basename(fpath)\n",
        "        group_id = fname.split(\"_\")[0] if \"_\" in fname else fname.split(\".\")[0]\n",
        "        rows.append([fpath, label, group_id])\n",
        "\n",
        "df = pd.DataFrame(rows, columns=[\"filepath\", \"label\", \"group\"])\n",
        "print(\"\\n Total samples loaded:\", len(df))\n",
        "print(df.head())\n",
        "\n",
        "if len(df) == 0:\n",
        "    raise ValueError(\" No image files found. Check dataset structure or path.\")\n",
        "\n",
        "# ===============================\n",
        "# STEP 3: Class distribution check\n",
        "# ===============================\n",
        "from collections import Counter\n",
        "\n",
        "class_counts = Counter(df['label'])\n",
        "print(\"\\n Class distribution:\")\n",
        "for cls, count in class_counts.items():\n",
        "    print(f\"{cls}: {count}\")\n",
        "\n",
        "# ===============================\n",
        "# STEP 4: Grouped train-test split (to avoid leakage)\n",
        "# ===============================\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "gss = GroupShuffleSplit(test_size=0.2, n_splits=1, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(df['filepath'], df['label'], groups=df['group']))\n",
        "\n",
        "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
        "test_df = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "print(\"\\n Split summary:\")\n",
        "print(\"Train size:\", len(train_df))\n",
        "print(\"Test size:\", len(test_df))\n",
        "\n",
        "# ===============================\n",
        "# STEP 5: Augmentation Probe (Safe vs Harmful)\n",
        "# ===============================\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pick a random training image\n",
        "sample_path = train_df['filepath'].sample(1, random_state=42).iloc[0]\n",
        "img = Image.open(sample_path)\n",
        "\n",
        "augmentations = {\n",
        "    \"RandomCrop\": T.RandomResizedCrop(size=244, scale=(0.8, 1.0)),\n",
        "    \"HorizontalFlip\": T.RandomHorizontalFlip(p=1.0),\n",
        "    \"ColorJitter\": T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    \"GaussianBlur\": T.GaussianBlur(kernel_size=5),\n",
        "    \"HeavyRotation\": T.RandomRotation(degrees=90),  # potential harmful\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, len(augmentations)+1, figsize=(18,5))\n",
        "axes[0].imshow(img)\n",
        "axes[0].set_title(\"Original\")\n",
        "axes[0].axis(\"off\")\n",
        "\n",
        "for i, (name, aug) in enumerate(augmentations.items(), 1):\n",
        "    transformed = aug(img)\n",
        "    axes[i].imshow(transformed)\n",
        "    axes[i].set_title(name)\n",
        "    axes[i].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n Augmentation probe complete — visually inspect which transforms help or hurt.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "generative_ai_disabled": true,
      "mount_file_id": "13oGTPCmf9TxAb8CxqafYJ7wGymJ5-Hje",
      "authorship_tag": "ABX9TyNEyJdNCSRqIb7sp7WcQyaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}